{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script untuk Download Data GDELT sampai ke Dataframe\n",
    "Script ini bersumber dari <a href=http://nbviewer.jupyter.org/github/JamesPHoughton/Published_Blog_Scripts/blob/master/GDELT%20Wrangler%20-%20Clean.ipynb>James P. Houghton</a> dengan sedikit modifikasi pada beberapa bagian."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Identifikasi File yang akan kita download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impor modul yang dibutuhkan\n",
    "import requests\n",
    "import lxml.html as lh\n",
    "import os\n",
    "\n",
    "# link yang berisi list url file yang akan di download\n",
    "gdelt_base_url = 'http://data.gdeltproject.org/events/'\n",
    "\n",
    "# periode data yang akan di download, dapat diganti sesuai kebutuhan\n",
    "begin_date = '20181001'\n",
    "end_date = '20181031'\n",
    "\n",
    "if begin_date < '200600':\n",
    "    begin_date = begin_date[0:4]\n",
    "elif begin_date < '20130401':\n",
    "    begin_date = begin_date[0:6]\n",
    "\n",
    "if end_date < '200600':\n",
    "    end_date = end_date[0:4]+'.zip'\n",
    "    \n",
    "print(begin_date)\n",
    "print(end_date)\n",
    "\n",
    "# mengambil list seluruh link yang ada pada halaman file gdelt\n",
    "page = requests.get(gdelt_base_url+'index.html')\n",
    "doc = lh.fromstring(page.content)\n",
    "link_list = doc.xpath(\"//*/ul/li/a/@href\")\n",
    "\n",
    "# menyeleksi/memfilter link sesuai dengan periode yang kita inginkan\n",
    "file_list = [x for x in link_list if str.isdigit(x[0:4])]\n",
    "file_list = [y for y in file_list if end_date >= str(y[0:8]) >= begin_date]\n",
    "\n",
    "# sebagai kontrol untuk link dan periode yang telah kita tentukan\n",
    "print(file_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Mengekstrak baris pada GDELT ke file perantara (intermediate files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Di tahap ini, kita memfilter baris data yang akan kita ekstrak berdasarkan kode negara tertentu, dan membuat file perantara baru yang merupakan copy dari file GDELT (input file) yang didownload. File perantara tersebut memiliki ukuran yang <b>lebih kecil</b> dari file input nya karena telah mengalami penyeleksian berdasarkan filter kode negara yang kita tentukan.</p>\n",
    "\n",
    "Kode negara yang merupakan parameter yang akan kita syaratkan pada row yang akan di ekstrak didasarkan pada 3 kolom yaitu:\n",
    "1. ActionGeo_CountryCode (51)\n",
    "2. Actor1Geo_CountryCode (37)\n",
    "3. Actor2Geo_CountryCode (44)\n",
    "Daftar kode negara dapat dilihat pada link berikut, <a href=http://en.wikipedia.org/wiki/List_of_FIPS_country_codes>FIPS Country Code</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "infilecounter = 0\n",
    "outfilecounter = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Secara umum, algoritma yang akan kita gunakan adalah sebagai berikut:\n",
    "\n",
    "- Mendapatkan list file gdelt yang akan di download\n",
    "- Dari setiap elemen pada list\n",
    "> - Cek apakah file tersebut telah ada di direktori lokal kita\n",
    ">> - Jika belum, script akan mendownloadnya\n",
    ">> - Jika sudah, script akan langsung ke proses selanjutnya yaitu:\n",
    "> - Unzip file gdelt\n",
    "> - Menghasilkan masing-masing satu file CSV pada folder /tmp\n",
    ">> - Membuat dan membuka file output (TSV)\n",
    ">> - Dari setiap baris input file (CSV):\n",
    ">>> - Membaca string yang ada\n",
    ">>> - Memisahkan berdasarkan delimiternya yaitu 'tab'\n",
    ">>> - Cek parameter (Kode Negara) pada kolom yang ditentukan:\n",
    ">>>> - Jika tidak memenuhi, lanjutkan looping\n",
    ">>>> - Jika memenuhi, tulis baris tersebut pada output (TSV) file\n",
    ">> - Menambahkan hitungan jumlah file yang telah diproses\n",
    ">> - Menghapus input (CSV) file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "import urllib\n",
    "import zipfile\n",
    "import glob\n",
    "import operator\n",
    "\n",
    "local_path = '/Users/bram/Documents/Digital Talent Scholarship 2018/GDELT_Project/GDELT_Data/'\n",
    "os.makedirs(os.path.dirname(local_path), exist_ok=True)\n",
    "\n",
    "fips_country_code = 'ID'\n",
    "\n",
    "for compressed_file in file_list[infilecounter:]:\n",
    "    print (compressed_file),\n",
    "    \n",
    "    # Jika kita tidak memiliki file nya di direktori lokal, script akan mendownloadnya\n",
    "    while not os.path.isfile(local_path+compressed_file): \n",
    "        print ('downloading,'),\n",
    "        urllib.request.urlretrieve(url=gdelt_base_url+compressed_file, \n",
    "                           filename=local_path+compressed_file)\n",
    "        \n",
    "    else:    \n",
    "        # Mengekstrak zip file ke folder sementara    \n",
    "        print ('extracting,'),\n",
    "        z = zipfile.ZipFile(file=local_path+compressed_file, mode='r')    \n",
    "        z.extractall(path=local_path+'tmp/')\n",
    "    \n",
    "    # Menguraikan tiap file csv ke working directory, \n",
    "    print ('parsing,'),\n",
    "    for infile_name in glob.glob(local_path+'tmp/*'):\n",
    "        if fips_country_code == '':\n",
    "            outfile_name = local_path+'country/'+'%04i.tsv'%outfilecounter\n",
    "            os.makedirs(os.path.dirname(outfile_name), exist_ok=True)\n",
    "            \n",
    "        else:\n",
    "            outfile_name = local_path+'country/'+fips_country_code+'%04i.tsv'%outfilecounter\n",
    "            os.makedirs(os.path.dirname(outfile_name), exist_ok=True)\n",
    "        \n",
    "        # membuka infile dan outfile\n",
    "        with open(infile_name, mode='r', encoding=\"utf-8\") as infile, open(outfile_name, mode='w', encoding=\"utf-8\") as outfile:\n",
    "            for line in infile:\n",
    "                # mengekstrak baris sesuai dengan kode negara yang telah ditentukan\n",
    "                if fips_country_code == '':\n",
    "                    outfile.write(line)\n",
    "                    \n",
    "                else:\n",
    "                    if fips_country_code in operator.itemgetter(51, 37, 44)(line.split('\\t')):\n",
    "                        outfile.write(line)\n",
    "            outfilecounter +=1\n",
    "            \n",
    "        # menghapus file sementara\n",
    "        os.remove(infile_name)\n",
    "    infilecounter +=1\n",
    "    print ('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Membangun file perantara ke Pandas Dataframe\n",
    "<p>Algoritma yang digunakan cukup sederhana yaitu mebangun dataframe dari setiap file sementara yang dihasilkan dari proses sebelumnya (TSV) kemudian menyatukannya ke dalam satu dataframe yang besar, menyimpan dataframe tersebut lalu menghapus file sementara yang sudah tidak dibutuhkan.</p>\n",
    "<p>Untuk memberikan header pada dataframe kita, digunakan file pada link berikut:</p>\n",
    "<br><a href=http://gdeltproject.org/data/lookups/CSV.header.fieldids.xlsx>http://gdeltproject.org/data/lookups/CSV.header.fieldids.xlsx</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "# Mengambil nama kolom file GDELT dari file \n",
    "colnames = pd.read_excel(local_path+'CSV.header.fieldids.xlsx', sheet_name='Sheet1', \n",
    "                         index_col='Column ID', usecols=1)['Field Name']\n",
    "\n",
    "# Membangun dataframe dari setiap file perantara (TSV)\n",
    "if fips_country_code == '':\n",
    "    files = glob.glob(local_path+'country/'+'*')    \n",
    "    \n",
    "else:\n",
    "    files = glob.glob(local_path+'country/'+fips_country_code+'*')\n",
    "    \n",
    "DFlist = []\n",
    "for active_file in files:\n",
    "    print (active_file)\n",
    "    DFlist.append(pd.read_csv(active_file, sep='\\t', header=None, dtype=str,\n",
    "                              names=colnames, index_col=['GLOBALEVENTID']))\n",
    "\n",
    "# Menggabungkan masing-masing dataframe dari proses sebelumnya, dan membuat file backup\n",
    "print (\"merging the file-based dataframes and save a pickle backup...tunggu...\"),\n",
    "DF = pd.concat(DFlist)\n",
    "if fips_country_code == '':\n",
    "    DF.to_pickle(local_path+'backup'+'.pickle')  \n",
    "    \n",
    "else:\n",
    "    DF.to_pickle(local_path+'backup'+fips_country_code+'.pickle')     \n",
    "    \n",
    "# Menghapus file sementara\n",
    "for active_file in files:\n",
    "    os.remove(active_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Mengekspor dataframe ke database MySQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Koneksi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymysql #mengimport module pymsql\n",
    "import pandas as pd #mengimport module pandas\n",
    "from sqlalchemy import create_engine #import modul create_engine dari library sqlalchemy\n",
    "\n",
    "#settingan database\n",
    "host = '127.0.0.1'   #ip database\n",
    "port = \"3306\"        #port database\n",
    "username = 'root'    #username\n",
    "password = ''        #password\n",
    "database = 'gdeltproject' #nama database\n",
    "\n",
    "#deklarasi koneksi ke database\n",
    "engine = create_engine('mysql+pymysql://'+username+':'+password+'@'+host+':'+port+'/'+database)\n",
    "# engine = create_engine('mysql+pymysql://root@localhost:3306/classicmodels')\n",
    "\n",
    "#deklarasi fungsi run\n",
    "def run(sql):\n",
    "    query = pd.read_sql_query(sql, engine)\n",
    "    return query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proses ekspor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF.to_sql(name= 'events', con = engine, if_exists = 'replace', index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Catatan:\n",
    "Mohon dicek jika ada bug dan semacamnya..\n",
    "<br>Sudah saya coba sampai proses ekspor ke database MySQL berjalan lancar, dengan catatan data tidak terlalu banyak..\n",
    "<br>Saya tes pake data events GDELT bulan Oktober 2018 dengan kode negara ID, file backupID.pickle yang dihasilkan hanya sekitar 16 MB, sampai ke MySQL, lancar jaya...\n",
    "<br>Tapi ketika data tidak difilter, file backup.pickle yang dihasilkan sekitar 2 GB, laptop saya (I7-8750H, RAM 8 GB) nggak sanggup...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
